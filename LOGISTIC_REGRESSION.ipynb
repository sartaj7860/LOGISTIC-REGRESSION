{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54gF4ruADRUx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION 1: What is logistic regression,and how does it differ from linear regression?\n",
        "ans) Logistic Regression\n",
        "\n",
        "Logistic regression is a statistical model used for binary classification problems, where the target variable is categorical (typically 0/1, yes/no, true/false). It's a popular algorithm in machine learning for predicting the probability of an event occurring based on a set of input variables.\n",
        "\n",
        "## Key Characteristics of Logistic Regression:\n",
        "1. Binary Outcome: Logistic regression is primarily used for binary classification (two classes).\n",
        "2. Probability Prediction: The model predicts probabilities of belonging to a particular class.\n",
        "3. Logistic Function (Sigmoid): The logistic function (sigmoid) is used to map predictions to probabilities between 0 and 1.\n",
        "4. Maximum Likelihood Estimation: Parameters are often estimated using maximum likelihood estimation.\n",
        "\n",
        "## Linear Regression vs. Logistic Regression:\n",
        "1. Outcome Variable:\n",
        "    - Linear Regression: Continuous outcome variable.\n",
        "    - Logistic Regression: Categorical (binary) outcome variable.\n",
        "2. Model Objective:\n",
        "    - Linear Regression: Predicts a continuous value by minimizing mean squared error.\n",
        "    - Logistic Regression: Predicts probability of a binary outcome using logistic function.\n",
        "3. Link Function:\n",
        "    - Linear Regression: Identity link (direct relationship).\n",
        "    - Logistic Regression: Logit link (logistic/sigmoid function for probabilities).\n",
        "4. Interpretation:\n",
        "    - Linear Regression: Coefficients represent change in outcome for a unit change in predictor.\n",
        "    - Logistic Regression: Coefficients represent change in log-odds for a unit change in predictor.\n",
        "5. Use Cases:\n",
        "    - Linear Regression: Predicting house prices, stock prices.\n",
        "    - Logistic Regression: Predicting spam emails, disease diagnosis (yes/no), customer churn.\n",
        "\n",
        "## Example:\n",
        "- Linear Regression: Predicting house price (continuous) based on features like area, bedrooms.\n",
        "- Logistic Regression: Predicting whether a customer will buy a product (yes/no) based on age, income, etc.\n",
        "\n",
        "## Considerations:\n",
        "- Logistic regression assumes independence of observations and linearity of log-odds with predictors.\n",
        "- Extensions like multinomial logistic regression handle more than two classes.\n",
        "\n",
        " QUESTION 2) Explain the role of the Sigmoid function in logistic regression.\n",
        " and) Sigmoid Function in Logistic Regression\n",
        "\n",
        "The Sigmoid function plays a crucial role in logistic regression, enabling the model to predict probabilities for binary classification problems.\n",
        "\n",
        "## Key Aspects of the Sigmoid Function:\n",
        "1. Definition: The Sigmoid function, also known as the logistic function, is defined as:\n",
        "\\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\]\n",
        "where \\( z \\) is the input (often a linear combination of features and weights).\n",
        "2. Shape: The Sigmoid function has an S-shaped curve, mapping any real-valued number to a value between 0 and 1.\n",
        "3. Probability Interpretation: The output of the Sigmoid function represents a probability, making it suitable for binary classification.\n",
        "4. Decision Boundary: A threshold (commonly 0.5) is used to make class predictions based on the probability output.\n",
        "\n",
        "## Role in Logistic Regression:\n",
        "1. Mapping Linear Combination to Probability: In logistic regression, \\( z = w^T x + b \\) (linear combination of inputs \\( x \\), weights \\( w \\), and bias \\( b \\)). The Sigmoid function maps \\( z \\) to a probability:\n",
        "\\[P(y=1|x) = \\sigma(w^T x + b)\\]\n",
        "2. Binary Classification: The model predicts class 1 if \\( \\sigma(z) \\geq 0.5 \\) (typically), otherwise class 0.\n",
        "3. Gradient for Learning: The Sigmoid function's properties are used in gradient-based optimization for training logistic regression models.\n",
        "\n",
        "## Properties of Sigmoid:\n",
        "- Range: Output is bounded between 0 and 1, suitable for probabilities.\n",
        "- Differentiable: The Sigmoid function is differentiable, facilitating gradient descent optimization.\n",
        "- Saturation: Sigmoid saturates at extremes (very high/low inputs), which can lead to vanishing gradients in some contexts.\n",
        "\n",
        "## Example:\n",
        "- For predicting email spam (1) vs. not spam (0), logistic regression with Sigmoid outputs the probability of an email being spam given features like words, sender.\n",
        "\n",
        "## Considerations:\n",
        "- Interpretation: Model coefficients indicate change in log-odds; odds ratios are often used for interpretation.\n",
        "- Alternatives: Other functions like tanh or softmax (for multiclass) exist; Sigmoid is common for binary logistic regression.\n",
        "\n",
        " QUESTION 3) What is the Regularization in logistic regression and why is it needed?\n",
        " ans) Regularization in Logistic Regression ðŸ“Š\n",
        "\n",
        "Regularization is a technique used in logistic regression (and other models) to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "## Why is Regularization Needed?\n",
        "1. Prevent Overfitting: Without regularization, logistic regression can overfit the training data, especially when there are many features or the dataset is small.\n",
        "2. Improve Generalization: Regularization helps improve the model's performance on unseen data by reducing complexity.\n",
        "3. Handle Multicollinearity: Regularization can help when features are highly correlated.\n",
        "\n",
        "## Types of Regularization:\n",
        "1. L1 Regularization (Lasso): Adds penalty proportional to the absolute value of coefficients (\\( |\\beta| \\)).\n",
        "    - Can lead to sparse models (some coefficients become exactly zero).\n",
        "2. L2 Regularization (Ridge): Adds penalty proportional to the square of coefficients (\\( \\beta^2 \\)).\n",
        "    - Shrinks coefficients but doesn't set them to zero typically.\n",
        "3. Elastic Net: Combination of L1 and L2 regularization.\n",
        "\n",
        "## How Regularization Works:\n",
        "- Penalty Term: A penalty term is added to the log-likelihood loss function being optimized in logistic regression.\n",
        "- Control via Hyperparameter: The strength of regularization is controlled by a hyperparameter (often denoted as \\( \\lambda \\) or \\( C \\) in some implementations).\n",
        "- Effect on Coefficients: Regularization shrinks coefficients towards zero, reducing model complexity.\n",
        "\n",
        "## Benefits:\n",
        "- Reduce Overfitting: Helps when model is complex relative to data.\n",
        "- Feature Selection (L1): L1 can drive some coefficients to zero, effectively selecting features.\n",
        "- Stability: Can improve stability in presence of correlated predictors.\n",
        "\n",
        "## Considerations:\n",
        "- Choice of Regularization: Choice depends on problem; L1 for sparsity, L2 for shrinkage.\n",
        "- Hyperparameter Tuning: Regularization strength often tuned via cross-validation.\n",
        "- Interpretation: Regularization affects interpretation of coefficients.\n",
        "\n",
        "## Example:\n",
        "- In a logistic regression predicting disease presence with many genetic markers, L1 regularization might help select relevant markers.\n",
        "\n",
        "## Common Implementations:\n",
        "- Libraries like scikit-learn (Python) provide logistic regression with regularization options.\n",
        "\n",
        " QUESTION 4) What are some common evaluation metrics for classfication models,and why are they important?\n",
        " ans) Regularization in Logistic Regression ðŸ“Š\n",
        "\n",
        "Regularization is a technique used in logistic regression (and other models) to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "## Why is Regularization Needed?\n",
        "1. Prevent Overfitting: Without regularization, logistic regression can overfit the training data, especially when there are many features or the dataset is small.\n",
        "2. Improve Generalization: Regularization helps improve the model's performance on unseen data by reducing complexity.\n",
        "3. Handle Multicollinearity: Regularization can help when features are highly correlated.\n",
        "\n",
        "## Types of Regularization:\n",
        "1. L1 Regularization (Lasso): Adds penalty proportional to the absolute value of coefficients (\\( |\\beta| \\)).\n",
        "    - Can lead to sparse models (some coefficients become exactly zero).\n",
        "2. L2 Regularization (Ridge): Adds penalty proportional to the square of coefficients (\\( \\beta^2 \\)).\n",
        "    - Shrinks coefficients but doesn't set them to zero typically.\n",
        "3. Elastic Net: Combination of L1 and L2 regularization.\n",
        "\n",
        "## How Regularization Works:\n",
        "- Penalty Term: A penalty term is added to the log-likelihood loss function being optimized in logistic regression.\n",
        "- Control via Hyperparameter: The strength of regularization is controlled by a hyperparameter (often denoted as \\( \\lambda \\) or \\( C \\) in some implementations).\n",
        "- Effect on Coefficients: Regularization shrinks coefficients towards zero, reducing model complexity.\n",
        "\n",
        "## Benefits:\n",
        "- Reduce Overfitting: Helps when model is complex relative to data.\n",
        "- Feature Selection (L1): L1 can drive some coefficients to zero, effectively selecting features.\n",
        "- Stability: Can improve stability in presence of correlated predictors.\n",
        "\n",
        "## Considerations:\n",
        "- Choice of Regularization: Choice depends on problem; L1 for sparsity, L2 for shrinkage.\n",
        "- Hyperparameter Tuning: Regularization strength often tuned via cross-validation.\n",
        "- Interpretation: Regularization affects interpretation of coefficients.\n",
        "\n",
        "## Example:\n",
        "- In a logistic regression predicting disease presence with many genetic markers, L1 regularization might help select relevant markers.\n",
        "\n",
        "## Common Implementations:\n",
        "- Libraries like scikit-learn (Python) provide logistic regression with regularization options.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_UrtQrgXDTwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9P86r9QGqjl",
        "outputId": "34805461-5988-4b81-9252-39601746822c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model using L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients (L2 Regularization):\\n\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVNY0N9EG98R",
        "outputId": "58a84c33-5cde-4a68-844b-852dbfbbdb5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (L2 Regularization):\n",
            "\n",
            "mean radius: 2.1325\n",
            "mean texture: 0.1528\n",
            "mean perimeter: -0.1451\n",
            "mean area: -0.0008\n",
            "mean smoothness: -0.1426\n",
            "mean compactness: -0.4156\n",
            "mean concavity: -0.6519\n",
            "mean concave points: -0.3445\n",
            "mean symmetry: -0.2076\n",
            "mean fractal dimension: -0.0298\n",
            "radius error: -0.0500\n",
            "texture error: 1.4430\n",
            "perimeter error: -0.3039\n",
            "area error: -0.0726\n",
            "smoothness error: -0.0162\n",
            "compactness error: -0.0019\n",
            "concavity error: -0.0449\n",
            "concave points error: -0.0377\n",
            "symmetry error: -0.0418\n",
            "fractal dimension error: 0.0056\n",
            "worst radius: 1.2321\n",
            "worst texture: -0.4046\n",
            "worst perimeter: -0.0362\n",
            "worst area: -0.0271\n",
            "worst smoothness: -0.2626\n",
            "worst compactness: -1.2090\n",
            "worst concavity: -1.6180\n",
            "worst concave points: -0.6153\n",
            "worst symmetry: -0.7428\n",
            "worst fractal dimension: -0.1170\n",
            "\n",
            "Model Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the iris dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model for multiclass classification using OvR\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfQOha-oG9s6",
        "outputId": "60082c74-0fce-4764-d28e-b96cb16dac29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      1.00      1.00         9\n",
            "   virginica       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer # Using Breast Cancer dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset from sklearn\n",
        "print(\"Loading Breast Cancer dataset from sklearn...\")\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Classes: {cancer.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "\n",
        "# --- Train model WITHOUT scaling ---\n",
        "print(\"\\nTraining Logistic Regression model WITHOUT scaling...\")\n",
        "model_no_scale = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear') # Use a suitable solver\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "accuracy_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scale:.4f} ({accuracy_no_scale*100:.2f}%)\")\n",
        "\n",
        "# --- Train model WITH scaling ---\n",
        "print(\"\\nStandardizing features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Training Logistic Regression model WITH scaling...\")\n",
        "model_scaled = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy WITH scaling:    {accuracy_scaled:.4f} ({accuracy_scaled*100:.2f}%)\")\n",
        "\n",
        "# --- Comparison ---\n",
        "print(\"\\nComparison of Accuracy:\")\n",
        "print(f\"  Without Scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"  With Scaling:    {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ47GPL1G9fA",
        "outputId": "fae62dce-aba3-4e0e-9923-67236ed706ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Breast Cancer dataset from sklearn...\n",
            "Dataset shape: (569, 30)\n",
            "Classes: ['malignant' 'benign']\n",
            "Class distribution: [212 357]\n",
            "\n",
            "Training set shape: (455, 30)\n",
            "Test set shape: (114, 30)\n",
            "\n",
            "Training Logistic Regression model WITHOUT scaling...\n",
            "Accuracy WITHOUT scaling: 0.9561 (95.61%)\n",
            "\n",
            "Standardizing features...\n",
            "Training Logistic Regression model WITH scaling...\n",
            "Accuracy WITH scaling:    0.9825 (98.25%)\n",
            "\n",
            "Comparison of Accuracy:\n",
            "  Without Scaling: 0.9561\n",
            "  With Scaling:    0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach youâ€™d take to build a Logistic Regression model â€” including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "ans)Approach for Building a Logistic Regression Model for Predicting Customer Response to Marketing Campaign ðŸ“Š\n",
        "\n",
        "Given the imbalanced dataset (5% responders), here's a structured approach for building a Logistic Regression model for this e-commerce business use case.\n",
        "\n",
        "## 1. Data Handling\n",
        "- Data Split: Split data into training (~70-80%), validation (~10-15%), and test (~10-15%) sets stratified by response variable to maintain class distribution.\n",
        "- Missing Values: Handle missing data appropriately (imputation, dropping if minimal and random).\n",
        "- Data Exploration: Understand distributions, correlations of features with response.\n",
        "\n",
        "## 2. Feature Selection and Engineering\n",
        "- Relevant Features: Select features likely impacting campaign response (demographics, past purchases, engagement metrics).\n",
        "- Feature Transformation: Consider transformations if needed for business interpretability or model fit.\n",
        "- Feature Scaling: Logistic Regression benefits from scaling (e.g., StandardScaler in Python) for regularization and some interpretive contexts.\n",
        "\n",
        "## 3. Handling Class Imbalance\n",
        "- Class Weighting: Use class weights in Logistic Regression (many implementations allow class_weight='balanced').\n",
        "- Oversampling/Undersampling: Alternatives like SMOTE for oversampling minority class; consider based on data and context.\n",
        "- Evaluation Metrics Focus: Focus on metrics like AUC-ROC, Precision-Recall, F1-score suited for imbalance.\n",
        "\n",
        "## 4. Model Training and Hyperparameter Tuning\n",
        "- Regularization: Apply L1 or L2 regularization to prevent overfitting; choice depends on feature set sparsity desires.\n",
        "- Hyperparameter Tuning: Use grid/random search with cross-validation (stratified) for parameters like C (inverse regularization strength).\n",
        "- Python Example: Use LogisticRegression from sklearn.linear_model with tuning via GridSearchCV.\n",
        "\n",
        "## 5. Model Evaluation\n",
        "- Metrics: Evaluate using AUC-ROC, Precision-Recall curve, F1-score, considering business cost of FP/FN.\n",
        "- Confusion Matrix: Useful for understanding at chosen threshold.\n",
        "- Business Context: Evaluate lift in targeting responders vs. random targeting; consider profit/cost implications.\n",
        "- Threshold Adjustment: Adjust decision threshold based on precision-recall trade-off fitting business goals.\n",
        "\n",
        "## 6. Interpretation and Business Actionability\n",
        "- Coefficients: Interpret coefficients for feature impact on log-odds of response; odds ratios useful.\n",
        "- Business Insights: Identify actionable segments; profile likely responders.\n",
        "- Campaign Targeting: Use model for targeting likely responders, balancing cost and uplift.\n",
        "\n",
        "## 7. Monitoring and Iteration\n",
        "- Model Monitoring: Track performance over time; data drift may necessitate retraining.\n",
        "- A/B Testing: Consider campaign A/B tests validating model-driven targeting.\n",
        "\n",
        "## Python Example Snippet\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assume X, y are data and target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Class imbalance handling via class_weight\n",
        "logreg = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='roc_auc')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_proba))\n",
        "## Considerations\n",
        "- Business Goals Alignment: Metrics and targeting strategy align with business goals (cost of targeting, campaign ROI).\n",
        "- Explainability: Model insights should aid business decisions; logistic regression coefficients are interpretable.\n",
        "- Ethical Use: Ensure use respects customer privacy and adheres to regulations (like GDPR).\n",
        "\n"
      ],
      "metadata": {
        "id": "Ip8ZCpdWDhsb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wqv96z7-JCDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}